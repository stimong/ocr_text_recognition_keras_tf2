{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from functools import reduce\n",
    "from tensorflow import keras\n",
    "import efficientnet.tfkeras as efficientnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Conv1D, Conv2D, MaxPool2D, BatchNormalization, LSTM, GRU\n",
    "from tensorflow.keras.layers import Reshape, Permute, Lambda, Bidirectional\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras import layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustumBatchNormalization(tf.keras.layers.BatchNormalization):\n",
    "    \"\"\"\n",
    "    Identical to keras.layers.BatchNormalization, but adds the option to freeze parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, freeze, *args, **kwargs):\n",
    "        self.freeze = freeze\n",
    "        super(CustumBatchNormalization, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # set to non-trainable if freeze is true\n",
    "        self.trainable = not self.freeze\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        # return super.call, but set training\n",
    "        if not training:\n",
    "            return super(CustumBatchNormalization, self).call(inputs, training=False)\n",
    "        else:\n",
    "            return super(CustumBatchNormalization, self).call(inputs, training=(not self.freeze))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustumBatchNormalization, self).get_config()\n",
    "        config.update({'freeze': self.freeze})\n",
    "        return config\n",
    "\n",
    "\n",
    "def DepthwiseConvBlock(kernel_size, strides, name, freeze_bn=False):\n",
    "    f1 = layers.DepthwiseConv2D(kernel_size=kernel_size, strides=strides, padding='same',\n",
    "                                use_bias=False, name='{}_dconv'.format(name))\n",
    "    f2 = CustumBatchNormalization(freeze=freeze_bn, name='{}_bn'.format(name))\n",
    "    f3 = layers.ReLU(name='{}_relu'.format(name))\n",
    "    return reduce(lambda f, g: lambda *args, **kwargs: g(f(*args, **kwargs)), (f1, f2, f3))\n",
    "\n",
    "\n",
    "def ConvBlock(num_channels, kernel_size, strides, name, freeze_bn=False):\n",
    "    f1 = layers.Conv2D(num_channels, kernel_size=kernel_size, strides=strides, padding='same',\n",
    "                       use_bias=False, name='{}_conv'.format(name))\n",
    "    f2 = CustumBatchNormalization(freeze=freeze_bn, name='{}_bn'.format(name))\n",
    "    f3 = layers.ReLU(name='{}_relu'.format(name))\n",
    "    return reduce(lambda f, g: lambda *args, **kwargs: g(f(*args, **kwargs)), (f1, f2, f3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RecogBaseModel():\n",
    "    def __init__(self,weights_path: str = None,backbone_name='vgg'):\n",
    "        self.weights_path=weights_path\n",
    "        self.backbone_name = backbone_name\n",
    "\n",
    "\n",
    "    def upconv(self, x, n, filters):\n",
    "        x = keras.layers.Conv2D(filters=filters, kernel_size=1, strides=1, name=f'upconv{n}.conv.0')(x)\n",
    "        x = keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.9, name=f'upconv{n}.conv.1')(x)\n",
    "        x = keras.layers.Activation('relu', name=f'upconv{n}.conv.2')(x)\n",
    "        x = keras.layers.Conv2D(filters=filters // 2,\n",
    "                                kernel_size=3,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                name=f'upconv{n}.conv.3')(x)\n",
    "        x = keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.9, name=f'upconv{n}.conv.4')(x)\n",
    "        x = keras.layers.Activation('relu', name=f'upconv{n}.conv.5')(x)\n",
    "        return x\n",
    "    \n",
    "    def build_efficientnet_backbone(self, inputs, backbone_name, imagenet):\n",
    "        backbone = getattr(efficientnet, backbone_name)(include_top=False,\n",
    "                                                        input_tensor=inputs,\n",
    "                                                        weights=None)#'imagenet' if imagenet else None)\n",
    "        return [\n",
    "            backbone.get_layer(slice_name).output for slice_name in [\n",
    "                'block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation',\n",
    "                'block5a_expand_activation'\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    def build_keras_model(self, inputs):#, weights_path: str = None, backbone_name='vgg'):\n",
    "        weights_path= self.weights_path\n",
    "        backbone_name = self.backbone_name\n",
    "        #inputs = tf.keras.layers.Input((None, None, 3))\n",
    "        #inputs =\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        if backbone_name == 'vgg':\n",
    "            print(\"no vgg..please add code.\")\n",
    "            pass\n",
    "            #s1, s2, s3, s4 = self.build_vgg_backbone(inputs)\n",
    "        elif 'efficientnet' in backbone_name.lower():\n",
    "            s1, s2, s3, s4 = self.build_efficientnet_backbone(inputs=inputs,\n",
    "                                                         backbone_name=backbone_name,\n",
    "                                                         imagenet=None)#weights_path is None)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        s1 = keras.layers.Conv2D(filters=int(s1.shape[-1]), kernel_size=1, strides=1)(s1)\n",
    "        s2 = keras.layers.Conv2D(filters=int(s1.shape[-1]), kernel_size=1, strides=1)(s2)\n",
    "        s3 = keras.layers.Conv2D(filters=int(s1.shape[-1]), kernel_size=1, strides=1)(s3)\n",
    "        s4 = keras.layers.Conv2D(filters=int(s1.shape[-1]), kernel_size=1, strides=1)(s4)\n",
    "        \n",
    "        def bifpn_layer(x1,x2,x3,x4, ids=0,just_up=False):\n",
    "            # upsample\n",
    "            x4_U = layers.UpSampling2D()(x4)\n",
    "            P3_td = layers.Add()([x4_U, x3])\n",
    "            P3_td = layers.Activation('swish')(P3_td)  \n",
    "            P3_td = DepthwiseConvBlock(kernel_size=3, strides=1, freeze_bn=False, name='BiFPN_{}_U_P3'.format(ids))(P3_td)\n",
    "\n",
    "            x3_U = layers.UpSampling2D()(P3_td)\n",
    "            P2_td = layers.Add()([x3_U,x2])\n",
    "            P2_td = layers.Activation('swish')(P2_td)  \n",
    "            P2_td = DepthwiseConvBlock(kernel_size=3, strides=1, freeze_bn=False, name='BiFPN_{}_U_P2'.format(ids))(P2_td)\n",
    "\n",
    "            x2_U = layers.UpSampling2D()(P2_td)\n",
    "            P1_td = layers.Add()([x2_U,x1])\n",
    "            P1_td = layers.Activation('swish')(P1_td)  \n",
    "            P1_out = DepthwiseConvBlock(kernel_size=3, strides=1, freeze_bn=False, name='BiFPN_{}_U_P1'.format(ids))(P1_td)\n",
    "\n",
    "            #print(\"P3_td.shape:{},P2_td.shape:{},P1_td.shape:{} , P1_out:{}\".format(P3_td.shape,P2_td.shape,P1_td.shape,P1_out.shape))\n",
    "\n",
    "            if just_up:\n",
    "                return P1_out, None, None, None\n",
    "            else:\n",
    "                # downsample\n",
    "                P1_D = layers.MaxPooling2D(strides=(2, 2))(P1_out)\n",
    "                P2_out = layers.Add()([P1_D, P2_td, x2])\n",
    "                P2_out = layers.Activation('swish')(P2_out)  \n",
    "                P2_out = DepthwiseConvBlock(kernel_size=3, strides=1, freeze_bn=False, name='BiFPN_{}_D_P1'.format(ids))(P2_out)\n",
    "\n",
    "\n",
    "                P2_D = layers.MaxPooling2D(strides=(2, 2))(P2_out)\n",
    "                P3_out = layers.Add()([P2_D,P3_td, x3])\n",
    "                P3_out = layers.Activation('swish')(P3_out)  \n",
    "                P3_out = DepthwiseConvBlock(kernel_size=3, strides=1, freeze_bn=False, name='BiFPN_{}_D_P2'.format(ids))(P3_out)\n",
    "\n",
    "                P3_D = layers.MaxPooling2D(strides=(2, 2))(P3_out)\n",
    "                P4_out = layers.Add()([P3_D, x4])\n",
    "                P4_out = layers.Activation('swish')(P4_out)  \n",
    "                P4_out = DepthwiseConvBlock(kernel_size=3, strides=1, freeze_bn=False, name='BiFPN_{}_D_P3'.format(ids))(P4_out)\n",
    "\n",
    "                return  P1_out, P2_out, P3_out, P4_out\n",
    "            \n",
    "        s1,s2,s3,s4 = bifpn_layer(s1,s2,s3,s4,ids=0, just_up=False)\n",
    "        s1,s2,s3,s4 = bifpn_layer(s1,s2,s3,s4,ids=1, just_up=False)\n",
    "        y,_,_,_ = bifpn_layer(s1,s2,s3,s4,ids=2, just_up=True)\n",
    " \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "text recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None, focal_ctc_on=False,  alpha=1, gamma=0.99):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "        self.focal_ctc_on = focal_ctc_on\n",
    "        self.alpha= alpha\n",
    "        self.gamma =gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        \n",
    "        print(\"tst ctc loss\")\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        ctc_loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        \n",
    "        if self.focal_ctc_on:\n",
    "            print(\"use focal_ctc_on\")\n",
    "            p = tf.exp(-ctc_loss)\n",
    "            focal_ctc_loss = self.alpha*tf.pow((1-p), self.gamma)*ctc_loss\n",
    "            self.add_loss(focal_ctc_loss)\n",
    "\n",
    "            # At test time, just return the computed predictions\n",
    "            \n",
    "        else:\n",
    "            self.add_loss(ctc_loss)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "class Attention(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_cell = BahdanauAttentionCell(hidden_size, num_classes)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.generator = Dense(num_classes)\n",
    "        \n",
    "    def call(self, x, text, is_train=True, batch_max_length=25):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_H = x=lstm: contextual_feature H = hidden state of encoder. [batch_size x num_steps x contextual_feature_channels]\n",
    "            text : the text-index of each image. [batch_size x (max_length+1)]. +1 for [GO] token. text[:, 0] = [GO].\n",
    "        output: probability distribution at each step [batch_size x num_steps x num_classes]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = tf.shape(x)[0]\n",
    "        num_steps = batch_max_length # +1 for [s] at end of sentence.\n",
    "\n",
    "        #output_hiddens = torch.FloatTensor(batch_size, num_steps, self.hidden_size).fill_(0).to(device)\n",
    "        hidden = ( tf.fill([batch_size, self.hidden_size],  np.float32(0)), \n",
    "                  tf.fill([batch_size, self.hidden_size],  np.float32(0)))\n",
    "        \n",
    "        \n",
    "        if is_train:\n",
    "            for i in range(num_steps):\n",
    "                char_onehots = tf.one_hot(text[:,i], depth=self.num_classes)\n",
    "                hidden, alpha = self.attention_cell(hidden, x, char_onehots)\n",
    "                \n",
    "                #print(\"hidden\")\n",
    "                reshape_hidden =tf.expand_dims(hidden[0], axis=1)\n",
    "\n",
    "                if i==0:\n",
    "                    output_hiddens = reshape_hidden\n",
    "                elif i>0:\n",
    "                    output_hiddens= tf.concat([output_hiddens, reshape_hidden], axis=1)\n",
    "                    \n",
    "            probs = self.generator(output_hiddens)\n",
    "            \n",
    "        else:\n",
    "            targets = tf.fill([batch_size,],  np.int32(0)) # [GO] token\n",
    "            for i in range(num_steps):\n",
    "                char_onehots = tf.one_hot(targets, depth=self.num_classes)\n",
    "                hidden, alpha = self.attention_cell(hidden, x, char_onehots)\n",
    "                \n",
    "                probs_step= self.generator(hidden[0])\n",
    "                next_input=tf.math.argmax(probs_step, axis=1)\n",
    "                targets = next_input\n",
    "                \n",
    "                \n",
    "                reshape_probs =tf.expand_dims(probs_step, axis=1)\n",
    "\n",
    "                if i==0:\n",
    "                    probs = reshape_probs\n",
    "                elif i>0:\n",
    "                    probs= tf.concat([probs, reshape_probs], axis=1)\n",
    "           \n",
    "                    \n",
    "        return probs\n",
    "                    \n",
    "            \n",
    "\n",
    "    \n",
    "class BahdanauAttentionCell(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_embeddings):\n",
    "        \"\"\"\n",
    "        you dont need input_size in tensorflow\n",
    "        num_embeddings: num_class\n",
    "        \"\"\"\n",
    "        super(BahdanauAttentionCell, self).__init__()\n",
    "        \n",
    "        self.i2h = Dense(hidden_size, use_bias=False)\n",
    "        self.h2h = Dense(hidden_size)\n",
    "        self.score = Dense(1, use_bias=False)\n",
    "        self.rnn = tf.keras.layers.LSTMCell(hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "\n",
    "    def call(self, prev_hidden, batch_H,  char_onehots): # 단, key와 value는 같음\n",
    "        batch_H_proj = self.i2h(batch_H)\n",
    "        prev_hidden_proj =tf.expand_dims(self.h2h(prev_hidden[0]), 1)\n",
    "        \n",
    "        e = self.score(tf.nn.tanh(batch_H_proj + prev_hidden_proj))\n",
    "        alpha = tf.nn.softmax(e, axis=1) \n",
    "        alpha = tf.keras.layers.Permute((2, 1))(alpha) \n",
    "        context = tf.matmul(alpha, batch_H)\n",
    "        \n",
    "        \n",
    "        context= tf.squeeze(context, 1)\n",
    "        concat_context=  tf.concat([context, char_onehots],1)\n",
    "        \n",
    "        cur_hidden = self.rnn(concat_context, prev_hidden)\n",
    "        return tuple(cur_hidden[1]), alpha\n",
    "    \n",
    "    \n",
    "\n",
    "def HeidiTextRecogModel(input_shape, num_classes, batch_max_length, backbone_name='EfficientNetB0',\n",
    "         prediction_mode='ctc',\n",
    "                        rnn_mode ='lstm',\n",
    "         \n",
    "         prediction_only=False, #gru=False, cnn=False,\n",
    "         hidden_size=256,\n",
    "         leaky_alpha=0.1,lstm_drop_rate=0.1,focal_ctc_on=False,alpha=0.75, gamma=0.5):\n",
    "    \"\"\"CRNN architecture.\n",
    "    \n",
    "    # Arguments\n",
    "        input_shape: Shape of the input image, (256, 32, 1).\n",
    "        num_classes: Number of characters in alphabet, including CTC blank.\n",
    "        \n",
    "    # References\n",
    "        https://arxiv.org/abs/1507.05717\n",
    "    \"\"\"\n",
    "    print(\"-\"*20)\n",
    "    print(\"input_shape: \",input_shape)\n",
    "    print(\"num_classes: \",num_classes)\n",
    "    print(\"batch_max_length: \",batch_max_length)\n",
    "    print(\"backbone_name: \",backbone_name)\n",
    "    print(\"prediction_mode: \",prediction_mode)\n",
    "    print(\"prediction_only: \",prediction_only)\n",
    "    print(\"rnn_mode: \",rnn_mode)\n",
    "    \n",
    "    act = LeakyReLU(alpha=leaky_alpha)\n",
    "    \n",
    "    input_img= layers.Input(\n",
    "        shape=input_shape, name=\"image\", dtype=\"float32\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    if backbone_name=='EfficientNetB0':\n",
    "        x=RecogBaseModel(weights_path=None, backbone_name='EfficientNetB0').build_keras_model(inputs=input_img)\n",
    "        \n",
    "        \n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "    \n",
    "    new_shape = (x.get_shape()[1],x.get_shape()[2]*x.get_shape()[3])\n",
    "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "    \n",
    "    x = layers.Dense(64, name=\"dense1\")(x)\n",
    "    x = act(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    if 'cnn' in rnn_mode:\n",
    "        for i in range(6):\n",
    "            x = BatchNormalization()(x)\n",
    "            x1 = Conv1D(128, 5, strides=1, dilation_rate=1, padding='same')(x)\n",
    "            x = act(x)\n",
    "            x2 = Conv1D(128, 5, strides=1, dilation_rate=2, padding='same')(x)\n",
    "            x = act(x)\n",
    "            x = concatenate([x1,x2])\n",
    "    elif 'gru' in rnn_mode:\n",
    "        x = Bidirectional(GRU(128,dropout=lstm_drop_rate,recurrent_dropout=lstm_drop_rate, return_sequences=True, reset_after=False))(x)\n",
    "        x = act(x)\n",
    "        x = Bidirectional(GRU(128,dropout=lstm_drop_rate,recurrent_dropout=lstm_drop_rate, return_sequences=True, reset_after=False))(x)\n",
    "        x = act(x)\n",
    "        \n",
    "    elif 'lstm' in rnn_mode:\n",
    "        x = Bidirectional(LSTM(128,dropout=lstm_drop_rate,recurrent_dropout=lstm_drop_rate, return_sequences=True, name='lstm_1'))(x)\n",
    "        x = act(x)\n",
    "        x = Bidirectional(LSTM(128,dropout=lstm_drop_rate,recurrent_dropout=lstm_drop_rate, return_sequences=True, name='lstm_2'))(x)\n",
    "        x = act(x)\n",
    "       \n",
    "\n",
    "    if 'ctc' in prediction_mode :\n",
    "        labels = layers.Input(name=\"label\", shape=(batch_max_length,), dtype=\"float32\")\n",
    "        x = Dense(num_classes)(x)\n",
    "        x  = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "        model_pred = Model(input_img, x)\n",
    "        if prediction_only:\n",
    "            return model_pred\n",
    "\n",
    "        # Add CTC layer for calculating CTC loss at each step\n",
    "        output = CTCLayer(name=\"ctc_loss\",focal_ctc_on=focal_ctc_on,  alpha=alpha, gamma=gamma)(labels, x)\n",
    "\n",
    "        # Define the model\n",
    "        model_train = keras.models.Model(\n",
    "            inputs=[input_img, labels], outputs=[output], name=\"ocr_model_v1\"\n",
    "        )\n",
    "        \n",
    "        print(\"output:\",output.shape)\n",
    "        \n",
    "    else:\n",
    "        labels = layers.Input(name=\"label\", shape=(batch_max_length,), dtype=\"int32\")\n",
    "        \n",
    "        #hidden_size = 64\n",
    "        attention = Attention(hidden_size, num_classes)\n",
    "        \n",
    "        \n",
    "        predict_probs= attention(x=x, text=None, is_train=False, batch_max_length=batch_max_length)\n",
    "        predict_x  = Activation('softmax', name='softmax_p')(predict_probs)\n",
    "        model_pred = keras.models.Model([input_img], predict_x)\n",
    "        \n",
    "        if prediction_only:\n",
    "            return model_pred\n",
    "        \n",
    "        train_probs= attention(x=x, text=labels, is_train=True, batch_max_length=batch_max_length)\n",
    "        train_x  = Activation('softmax', name='softmax_t')(train_probs)\n",
    "        \n",
    "        model_train = keras.models.Model([input_img,labels], train_x, name=\"ocr_model_v1\")\n",
    "        \n",
    "        \n",
    "        print(\"output:\",train_x.shape)\n",
    "        \n",
    "    return model_train, model_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heidi_loss(y_true_, y_pred_):\n",
    "    cce=tf.keras.losses.CategoricalCrossentropy()\n",
    "    all_loss=0\n",
    "    for yi in range(0,y_pred_.shape[1]):\n",
    "        y_true= y_true_[:,yi,:]\n",
    "        y_pred= y_pred_[:,yi,:]\n",
    "        \n",
    "        all_loss+= cce(y_true,y_pred)\n",
    "        \n",
    "    return all_loss\n",
    "\n",
    "def heidi_acc(y_true_, y_pred_):\n",
    "    all_acc=0\n",
    "    for yi in range(0,y_pred_.shape[1]):\n",
    "        y_true= y_true_[:,yi,:]\n",
    "        y_pred= y_pred_[:,yi,:]\n",
    "        \n",
    "        acc1 = K.mean(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))\n",
    "\n",
    "        all_acc+=acc1\n",
    "    return ((all_acc)/y_pred_.shape[1])\n",
    "\n",
    "def catg_loss(y_true, y_pred):\n",
    "    # scale predictions so that the class probas of each sample sum to 1\n",
    "    y_pred /= K.sum(y_pred, axis=[2], keepdims=True)\n",
    "    # clip to prevent NaN's and Inf's\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    \n",
    "    loss = y_true * K.log(y_pred)\n",
    "    loss = -K.sum(loss, [1,2])\n",
    "    return loss\n",
    "# def loss_fn(ytrue, ypred):\n",
    "#     return -K.sum(ytrue*K.log(ypred+1e-6), [1,2])\n",
    "\n",
    "\n",
    "\n",
    "def catg_acc(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))\n",
    "\n",
    "def soft_acc(y_true, y_pred):\n",
    "       return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "input_shape:  (64, 16, 1)\n",
      "num_classes:  17\n",
      "batch_max_length:  6\n",
      "backbone_name:  EfficientNetB0\n",
      "prediction_mode:  attn\n",
      "prediction_only:  False\n",
      "rnn_mode:  gru\n",
      "output: (None, 6, 17)\n"
     ]
    }
   ],
   "source": [
    "prediction_mode='attn'\n",
    "\n",
    "letters='abcdefg1234567890'\n",
    "max_text_len=6 #ctc loss, model ouptshape becomes max_text_len value.\n",
    "\n",
    "\n",
    "alpha=0.99\n",
    "gamma=1\n",
    "\n",
    "\n",
    "input_width, input_height = 64,16#200,50\n",
    "input_shape=(input_width,input_height,1)\n",
    "\n",
    "\n",
    "ocr_recog_model, ocr_recog_model_pred =HeidiTextRecogModel(input_shape, \n",
    "                                                          len(letters), \n",
    "                                                          max_text_len,\n",
    "                                                          backbone_name='EfficientNetB0',\n",
    "                                                          prediction_mode=prediction_mode,\n",
    "                                                          prediction_only=False, rnn_mode ='gru',\n",
    "                                                           hidden_size=256,\n",
    "                                                           leaky_alpha=0.2,lstm_drop_rate=0.5,\n",
    "                                                           focal_ctc_on=True, alpha=alpha, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ocr_model_v1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, 64, 16, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "stem_conv (Conv2D)              (None, 32, 8, 32)    288         image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stem_bn (BatchNormalization)    (None, 32, 8, 32)    128         stem_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "stem_activation (Activation)    (None, 32, 8, 32)    0           stem_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1a_dwconv (DepthwiseConv2D (None, 32, 8, 32)    288         stem_activation[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1a_bn (BatchNormalization) (None, 32, 8, 32)    128         block1a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block1a_activation (Activation) (None, 32, 8, 32)    0           block1a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_squeeze (GlobalAvera (None, 32)           0           block1a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_reshape (Reshape)    (None, 1, 1, 32)     0           block1a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_reduce (Conv2D)      (None, 1, 1, 8)      264         block1a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_expand (Conv2D)      (None, 1, 1, 32)     288         block1a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_excite (Multiply)    (None, 32, 8, 32)    0           block1a_activation[0][0]         \n",
      "                                                                 block1a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block1a_project_conv (Conv2D)   (None, 32, 8, 16)    512         block1a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block1a_project_bn (BatchNormal (None, 32, 8, 16)    64          block1a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block2a_expand_conv (Conv2D)    (None, 32, 8, 96)    1536        block1a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_expand_bn (BatchNormali (None, 32, 8, 96)    384         block2a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2a_expand_activation (Acti (None, 32, 8, 96)    0           block2a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2a_dwconv (DepthwiseConv2D (None, 16, 4, 96)    864         block2a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block2a_bn (BatchNormalization) (None, 16, 4, 96)    384         block2a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block2a_activation (Activation) (None, 16, 4, 96)    0           block2a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_squeeze (GlobalAvera (None, 96)           0           block2a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_reshape (Reshape)    (None, 1, 1, 96)     0           block2a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_reduce (Conv2D)      (None, 1, 1, 4)      388         block2a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_expand (Conv2D)      (None, 1, 1, 96)     480         block2a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_excite (Multiply)    (None, 16, 4, 96)    0           block2a_activation[0][0]         \n",
      "                                                                 block2a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2a_project_conv (Conv2D)   (None, 16, 4, 24)    2304        block2a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2a_project_bn (BatchNormal (None, 16, 4, 24)    96          block2a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block2b_expand_conv (Conv2D)    (None, 16, 4, 144)   3456        block2a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_expand_bn (BatchNormali (None, 16, 4, 144)   576         block2b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2b_expand_activation (Acti (None, 16, 4, 144)   0           block2b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2b_dwconv (DepthwiseConv2D (None, 16, 4, 144)   1296        block2b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block2b_bn (BatchNormalization) (None, 16, 4, 144)   576         block2b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block2b_activation (Activation) (None, 16, 4, 144)   0           block2b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_squeeze (GlobalAvera (None, 144)          0           block2b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_reshape (Reshape)    (None, 1, 1, 144)    0           block2b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_reduce (Conv2D)      (None, 1, 1, 6)      870         block2b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_expand (Conv2D)      (None, 1, 1, 144)    1008        block2b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_excite (Multiply)    (None, 16, 4, 144)   0           block2b_activation[0][0]         \n",
      "                                                                 block2b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2b_project_conv (Conv2D)   (None, 16, 4, 24)    3456        block2b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2b_project_bn (BatchNormal (None, 16, 4, 24)    96          block2b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block2b_drop (FixedDropout)     (None, 16, 4, 24)    0           block2b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_add (Add)               (None, 16, 4, 24)    0           block2b_drop[0][0]               \n",
      "                                                                 block2a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_expand_conv (Conv2D)    (None, 16, 4, 144)   3456        block2b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3a_expand_bn (BatchNormali (None, 16, 4, 144)   576         block3a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3a_expand_activation (Acti (None, 16, 4, 144)   0           block3a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3a_dwconv (DepthwiseConv2D (None, 8, 2, 144)    3600        block3a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block3a_bn (BatchNormalization) (None, 8, 2, 144)    576         block3a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block3a_activation (Activation) (None, 8, 2, 144)    0           block3a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_squeeze (GlobalAvera (None, 144)          0           block3a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_reshape (Reshape)    (None, 1, 1, 144)    0           block3a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_reduce (Conv2D)      (None, 1, 1, 6)      870         block3a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_expand (Conv2D)      (None, 1, 1, 144)    1008        block3a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_excite (Multiply)    (None, 8, 2, 144)    0           block3a_activation[0][0]         \n",
      "                                                                 block3a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3a_project_conv (Conv2D)   (None, 8, 2, 40)     5760        block3a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3a_project_bn (BatchNormal (None, 8, 2, 40)     160         block3a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block3b_expand_conv (Conv2D)    (None, 8, 2, 240)    9600        block3a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_expand_bn (BatchNormali (None, 8, 2, 240)    960         block3b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3b_expand_activation (Acti (None, 8, 2, 240)    0           block3b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3b_dwconv (DepthwiseConv2D (None, 8, 2, 240)    6000        block3b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block3b_bn (BatchNormalization) (None, 8, 2, 240)    960         block3b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block3b_activation (Activation) (None, 8, 2, 240)    0           block3b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_squeeze (GlobalAvera (None, 240)          0           block3b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_reshape (Reshape)    (None, 1, 1, 240)    0           block3b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block3b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block3b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_excite (Multiply)    (None, 8, 2, 240)    0           block3b_activation[0][0]         \n",
      "                                                                 block3b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3b_project_conv (Conv2D)   (None, 8, 2, 40)     9600        block3b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3b_project_bn (BatchNormal (None, 8, 2, 40)     160         block3b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block3b_drop (FixedDropout)     (None, 8, 2, 40)     0           block3b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_add (Add)               (None, 8, 2, 40)     0           block3b_drop[0][0]               \n",
      "                                                                 block3a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_expand_conv (Conv2D)    (None, 8, 2, 240)    9600        block3b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4a_expand_bn (BatchNormali (None, 8, 2, 240)    960         block4a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4a_expand_activation (Acti (None, 8, 2, 240)    0           block4a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4a_dwconv (DepthwiseConv2D (None, 4, 1, 240)    2160        block4a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block4a_bn (BatchNormalization) (None, 4, 1, 240)    960         block4a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block4a_activation (Activation) (None, 4, 1, 240)    0           block4a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_squeeze (GlobalAvera (None, 240)          0           block4a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_reshape (Reshape)    (None, 1, 1, 240)    0           block4a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block4a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block4a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_excite (Multiply)    (None, 4, 1, 240)    0           block4a_activation[0][0]         \n",
      "                                                                 block4a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4a_project_conv (Conv2D)   (None, 4, 1, 80)     19200       block4a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4a_project_bn (BatchNormal (None, 4, 1, 80)     320         block4a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4b_expand_conv (Conv2D)    (None, 4, 1, 480)    38400       block4a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_expand_bn (BatchNormali (None, 4, 1, 480)    1920        block4b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4b_expand_activation (Acti (None, 4, 1, 480)    0           block4b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4b_dwconv (DepthwiseConv2D (None, 4, 1, 480)    4320        block4b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block4b_bn (BatchNormalization) (None, 4, 1, 480)    1920        block4b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block4b_activation (Activation) (None, 4, 1, 480)    0           block4b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_squeeze (GlobalAvera (None, 480)          0           block4b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_reshape (Reshape)    (None, 1, 1, 480)    0           block4b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_reduce (Conv2D)      (None, 1, 1, 20)     9620        block4b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_expand (Conv2D)      (None, 1, 1, 480)    10080       block4b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_excite (Multiply)    (None, 4, 1, 480)    0           block4b_activation[0][0]         \n",
      "                                                                 block4b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4b_project_conv (Conv2D)   (None, 4, 1, 80)     38400       block4b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4b_project_bn (BatchNormal (None, 4, 1, 80)     320         block4b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4b_drop (FixedDropout)     (None, 4, 1, 80)     0           block4b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_add (Add)               (None, 4, 1, 80)     0           block4b_drop[0][0]               \n",
      "                                                                 block4a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_expand_conv (Conv2D)    (None, 4, 1, 480)    38400       block4b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4c_expand_bn (BatchNormali (None, 4, 1, 480)    1920        block4c_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4c_expand_activation (Acti (None, 4, 1, 480)    0           block4c_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4c_dwconv (DepthwiseConv2D (None, 4, 1, 480)    4320        block4c_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block4c_bn (BatchNormalization) (None, 4, 1, 480)    1920        block4c_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block4c_activation (Activation) (None, 4, 1, 480)    0           block4c_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_squeeze (GlobalAvera (None, 480)          0           block4c_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_reshape (Reshape)    (None, 1, 1, 480)    0           block4c_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_reduce (Conv2D)      (None, 1, 1, 20)     9620        block4c_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_expand (Conv2D)      (None, 1, 1, 480)    10080       block4c_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_excite (Multiply)    (None, 4, 1, 480)    0           block4c_activation[0][0]         \n",
      "                                                                 block4c_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4c_project_conv (Conv2D)   (None, 4, 1, 80)     38400       block4c_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4c_project_bn (BatchNormal (None, 4, 1, 80)     320         block4c_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4c_drop (FixedDropout)     (None, 4, 1, 80)     0           block4c_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_add (Add)               (None, 4, 1, 80)     0           block4c_drop[0][0]               \n",
      "                                                                 block4b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5a_expand_conv (Conv2D)    (None, 4, 1, 480)    38400       block4c_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5a_expand_bn (BatchNormali (None, 4, 1, 480)    1920        block5a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5a_expand_activation (Acti (None, 4, 1, 480)    0           block5a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 1, 96)     46176       block5a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 2, 96)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 2, 96)     23136       block4a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8, 2, 96)     0           up_sampling2d[0][0]              \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 8, 2, 96)     0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_U_P3_dconv (DepthwiseCo (None, 8, 2, 96)     864         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_U_P3_bn (CustumBatchNor (None, 8, 2, 96)     384         BiFPN_0_U_P3_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_U_P3_relu (ReLU)        (None, 8, 2, 96)     0           BiFPN_0_U_P3_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 4, 96)    0           BiFPN_0_U_P3_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 4, 96)    13920       block3a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 4, 96)    0           up_sampling2d_1[0][0]            \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 4, 96)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_U_P2_dconv (DepthwiseCo (None, 16, 4, 96)    864         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_U_P2_bn (CustumBatchNor (None, 16, 4, 96)    384         BiFPN_0_U_P2_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_U_P2_relu (ReLU)        (None, 16, 4, 96)    0           BiFPN_0_U_P2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 8, 96)    0           BiFPN_0_U_P2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 8, 96)    9312        block2a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 8, 96)    0           up_sampling2d_2[0][0]            \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 8, 96)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_U_P1_dconv (DepthwiseCo (None, 32, 8, 96)    864         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_U_P1_bn (CustumBatchNor (None, 32, 8, 96)    384         BiFPN_0_U_P1_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_U_P1_relu (ReLU)        (None, 32, 8, 96)    0           BiFPN_0_U_P1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 16, 4, 96)    0           BiFPN_0_U_P1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 4, 96)    0           max_pooling2d[0][0]              \n",
      "                                                                 BiFPN_0_U_P2_relu[0][0]          \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 4, 96)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_D_P1_dconv (DepthwiseCo (None, 16, 4, 96)    864         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_D_P1_bn (CustumBatchNor (None, 16, 4, 96)    384         BiFPN_0_D_P1_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_D_P1_relu (ReLU)        (None, 16, 4, 96)    0           BiFPN_0_D_P1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 8, 2, 96)     0           BiFPN_0_D_P1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 2, 96)     0           max_pooling2d_1[0][0]            \n",
      "                                                                 BiFPN_0_U_P3_relu[0][0]          \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 8, 2, 96)     0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_D_P2_dconv (DepthwiseCo (None, 8, 2, 96)     864         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_D_P2_bn (CustumBatchNor (None, 8, 2, 96)     384         BiFPN_0_D_P2_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_D_P2_relu (ReLU)        (None, 8, 2, 96)     0           BiFPN_0_D_P2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 4, 1, 96)     0           BiFPN_0_D_P2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 1, 96)     0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 4, 1, 96)     0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_D_P3_dconv (DepthwiseCo (None, 4, 1, 96)     864         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_D_P3_bn (CustumBatchNor (None, 4, 1, 96)     384         BiFPN_0_D_P3_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_0_D_P3_relu (ReLU)        (None, 4, 1, 96)     0           BiFPN_0_D_P3_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 8, 2, 96)     0           BiFPN_0_D_P3_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 2, 96)     0           up_sampling2d_3[0][0]            \n",
      "                                                                 BiFPN_0_D_P2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 2, 96)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_U_P3_dconv (DepthwiseCo (None, 8, 2, 96)     864         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_U_P3_bn (CustumBatchNor (None, 8, 2, 96)     384         BiFPN_1_U_P3_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_U_P3_relu (ReLU)        (None, 8, 2, 96)     0           BiFPN_1_U_P3_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 16, 4, 96)    0           BiFPN_1_U_P3_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 4, 96)    0           up_sampling2d_4[0][0]            \n",
      "                                                                 BiFPN_0_D_P1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 4, 96)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_U_P2_dconv (DepthwiseCo (None, 16, 4, 96)    864         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_U_P2_bn (CustumBatchNor (None, 16, 4, 96)    384         BiFPN_1_U_P2_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_U_P2_relu (ReLU)        (None, 16, 4, 96)    0           BiFPN_1_U_P2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 32, 8, 96)    0           BiFPN_1_U_P2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 32, 8, 96)    0           up_sampling2d_5[0][0]            \n",
      "                                                                 BiFPN_0_U_P1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 8, 96)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_U_P1_dconv (DepthwiseCo (None, 32, 8, 96)    864         activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_U_P1_bn (CustumBatchNor (None, 32, 8, 96)    384         BiFPN_1_U_P1_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_U_P1_relu (ReLU)        (None, 32, 8, 96)    0           BiFPN_1_U_P1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 4, 96)    0           BiFPN_1_U_P1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 4, 96)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 BiFPN_1_U_P2_relu[0][0]          \n",
      "                                                                 BiFPN_0_D_P1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 4, 96)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_D_P1_dconv (DepthwiseCo (None, 16, 4, 96)    864         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_D_P1_bn (CustumBatchNor (None, 16, 4, 96)    384         BiFPN_1_D_P1_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_D_P1_relu (ReLU)        (None, 16, 4, 96)    0           BiFPN_1_D_P1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 2, 96)     0           BiFPN_1_D_P1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 8, 2, 96)     0           max_pooling2d_4[0][0]            \n",
      "                                                                 BiFPN_1_U_P3_relu[0][0]          \n",
      "                                                                 BiFPN_0_D_P2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 2, 96)     0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_D_P2_dconv (DepthwiseCo (None, 8, 2, 96)     864         activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_D_P2_bn (CustumBatchNor (None, 8, 2, 96)     384         BiFPN_1_D_P2_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_D_P2_relu (ReLU)        (None, 8, 2, 96)     0           BiFPN_1_D_P2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 4, 1, 96)     0           BiFPN_1_D_P2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 4, 1, 96)     0           max_pooling2d_5[0][0]            \n",
      "                                                                 BiFPN_0_D_P3_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 4, 1, 96)     0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_D_P3_dconv (DepthwiseCo (None, 4, 1, 96)     864         activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_D_P3_bn (CustumBatchNor (None, 4, 1, 96)     384         BiFPN_1_D_P3_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_1_D_P3_relu (ReLU)        (None, 4, 1, 96)     0           BiFPN_1_D_P3_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 8, 2, 96)     0           BiFPN_1_D_P3_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 2, 96)     0           up_sampling2d_6[0][0]            \n",
      "                                                                 BiFPN_1_D_P2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 2, 96)     0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_2_U_P3_dconv (DepthwiseCo (None, 8, 2, 96)     864         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_2_U_P3_bn (CustumBatchNor (None, 8, 2, 96)     384         BiFPN_2_U_P3_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_2_U_P3_relu (ReLU)        (None, 8, 2, 96)     0           BiFPN_2_U_P3_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 16, 4, 96)    0           BiFPN_2_U_P3_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 4, 96)    0           up_sampling2d_7[0][0]            \n",
      "                                                                 BiFPN_1_D_P1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 4, 96)    0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_2_U_P2_dconv (DepthwiseCo (None, 16, 4, 96)    864         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_2_U_P2_bn (CustumBatchNor (None, 16, 4, 96)    384         BiFPN_2_U_P2_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_2_U_P2_relu (ReLU)        (None, 16, 4, 96)    0           BiFPN_2_U_P2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 32, 8, 96)    0           BiFPN_2_U_P2_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 32, 8, 96)    0           up_sampling2d_8[0][0]            \n",
      "                                                                 BiFPN_1_U_P1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 8, 96)    0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_2_U_P1_dconv (DepthwiseCo (None, 32, 8, 96)    864         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_2_U_P1_bn (CustumBatchNor (None, 32, 8, 96)    384         BiFPN_2_U_P1_dconv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "BiFPN_2_U_P1_relu (ReLU)        (None, 32, 8, 96)    0           BiFPN_2_U_P1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 16, 4, 96)    0           BiFPN_2_U_P1_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 16, 384)      0           pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 16, 64)       24640       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         multiple             0           dense1[0][0]                     \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16, 64)       0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 16, 256)      148224      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 16, 256)      295680      leaky_re_lu[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "label (InputLayer)              [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 6, 17)        678673      leaky_re_lu[2][0]                \n",
      "                                                                 label[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "softmax_t (Activation)          (None, 6, 17)        0           attention[1][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,615,077\n",
      "Trainable params: 1,603,045\n",
      "Non-trainable params: 12,032\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ocr_recog_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_text_len: 6\n",
      "img: (500, 64, 16),\n",
      "label: (500, 6),\n",
      "onehot: (500, 6, 17)\n"
     ]
    }
   ],
   "source": [
    "def get_sample_data(n, w, h, max_text_len):\n",
    "    \"\"\"\n",
    "    This is a sample data generation function.\n",
    "    If you have text image data, load it and use it.\n",
    "    \"\"\"\n",
    "    nb_classes = len(letters)\n",
    "    x = np.zeros((n, w, h))\n",
    "    y = np.full((n, max_text_len),nb_classes-1,dtype='int32')\n",
    "    \n",
    "    for ni in range(n):\n",
    "        randnum = np.random.randint(2,nb_classes-1)\n",
    "        y[ni,0]=randnum\n",
    "        y[ni,1]=randnum\n",
    "        y[ni,2]=randnum\n",
    "        y[ni,3]=randnum-1\n",
    "        x[ni, :, :] = randnum\n",
    "    x= x/len(letters)\n",
    "    \n",
    "    \n",
    "    targets = np.array(y)\n",
    "    one_hot_targets = np.eye(nb_classes)[targets]\n",
    "    \n",
    "    inputs={'image': x, \n",
    "                'label': y}\n",
    "    return inputs,one_hot_targets\n",
    "\n",
    "if prediction_mode =='ctc':\n",
    "    \"\"\"\n",
    "    ctc loss, ouptshape becomes max_text_len value.\n",
    "    \"\"\"\n",
    "    max_text_len=ocr_recog_model.output_shape[1]\n",
    "else:\n",
    "    max_text_len=max_text_len\n",
    "    \n",
    "    \n",
    "print(\"max_text_len:\",max_text_len)\n",
    "train_x, train_y = get_sample_data(500, input_width, input_height,max_text_len)\n",
    "print(\"img: {},\\nlabel: {},\\nonehot: {}\".format(train_x['image'].shape, train_x['label'].shape, train_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "model compile and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_radam.training import RAdamOptimizer\n",
    "opt=RAdamOptimizer(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prediction_mode =='ctc':\n",
    "    ocr_recog_model.compile(optimizer=opt)\n",
    "else:#attention loss\n",
    "    ocr_recog_model.compile(optimizer=opt,loss=catg_loss, metrics=[catg_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 155ms/step - loss: 16.7173 - catg_acc: 0.2844\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 16.2294 - catg_acc: 0.3333\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 15.2012 - catg_acc: 0.3333\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 13.6865 - catg_acc: 0.3333\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 13.1472 - catg_acc: 0.3333\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 12.6362 - catg_acc: 0.3333\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 12.1019 - catg_acc: 0.3333\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 11.2949 - catg_acc: 0.3362\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 10.3273 - catg_acc: 0.4321\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 8.9178 - catg_acc: 0.5507\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 7.5222 - catg_acc: 0.6682\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 6.0680 - catg_acc: 0.7502\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 4.8851 - catg_acc: 0.8101\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 4.2588 - catg_acc: 0.8042\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 3.9932 - catg_acc: 0.7915\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 3.2127 - catg_acc: 0.8722\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 2.8051 - catg_acc: 0.8943\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 1s 158ms/step - loss: 2.4681 - catg_acc: 0.9196\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 2.2818 - catg_acc: 0.9190\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 2.0518 - catg_acc: 0.9359\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 1.7787 - catg_acc: 0.9531\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 1.9874 - catg_acc: 0.9192\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 1.6183 - catg_acc: 0.9547\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 1.5573 - catg_acc: 0.9508\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 1.6232 - catg_acc: 0.9294\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 1.3914 - catg_acc: 0.9634\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 1.2357 - catg_acc: 0.9728\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 1.0620 - catg_acc: 0.9883\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.9198 - catg_acc: 0.9878\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 0.9690 - catg_acc: 0.9764\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.8067 - catg_acc: 0.9831\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.7545 - catg_acc: 0.9869\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.7448 - catg_acc: 0.9896\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.5783 - catg_acc: 0.9971\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.5401 - catg_acc: 0.9952\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.5019 - catg_acc: 0.9969\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 1s 151ms/step - loss: 0.4669 - catg_acc: 0.9941\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.4217 - catg_acc: 0.9964\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.4137 - catg_acc: 0.9971\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.3422 - catg_acc: 0.9988\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.3003 - catg_acc: 0.9997\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.2832 - catg_acc: 0.9979\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.2566 - catg_acc: 0.9997\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.2421 - catg_acc: 0.9990\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.2315 - catg_acc: 0.9987\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.1982 - catg_acc: 1.0000\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.4708 - catg_acc: 0.9802\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 1.1269 - catg_acc: 0.9281\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 1.3904 - catg_acc: 0.9094\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.9202 - catg_acc: 0.9549\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 1.4525 - catg_acc: 0.9027\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.8233 - catg_acc: 0.9709\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.6387 - catg_acc: 0.9805\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.4220 - catg_acc: 0.9951\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.2476 - catg_acc: 1.0000\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.2822 - catg_acc: 0.9958\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.1799 - catg_acc: 0.9993\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.2764 - catg_acc: 0.9967\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.1804 - catg_acc: 0.9993\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.1403 - catg_acc: 1.0000\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.1360 - catg_acc: 0.9993\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.1480 - catg_acc: 0.9981\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.1186 - catg_acc: 1.0000\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.1311 - catg_acc: 0.9993\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.1444 - catg_acc: 0.9987\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.1036 - catg_acc: 1.0000\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0918 - catg_acc: 1.0000\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.8002 - catg_acc: 0.9523\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.3929 - catg_acc: 0.9830\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 1s 151ms/step - loss: 0.2312 - catg_acc: 0.9970\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.7244 - catg_acc: 0.9663\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.3772 - catg_acc: 0.9814\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.3739 - catg_acc: 0.9847\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.1831 - catg_acc: 0.9977\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 1s 157ms/step - loss: 0.1306 - catg_acc: 0.9997\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.0887 - catg_acc: 1.0000\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.5440 - catg_acc: 0.9700\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.2037 - catg_acc: 0.9987\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.1211 - catg_acc: 0.9997\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.1056 - catg_acc: 0.9990\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0765 - catg_acc: 1.0000\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0700 - catg_acc: 0.9997\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0608 - catg_acc: 1.0000\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.2596 - catg_acc: 0.9857\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.1674 - catg_acc: 0.9974\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.1240 - catg_acc: 0.9990\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0727 - catg_acc: 1.0000\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.2092 - catg_acc: 0.9922\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.0970 - catg_acc: 1.0000\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.0602 - catg_acc: 1.0000\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0508 - catg_acc: 1.0000\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0467 - catg_acc: 1.0000\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0439 - catg_acc: 1.0000\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0408 - catg_acc: 1.0000\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0373 - catg_acc: 1.0000\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0356 - catg_acc: 1.0000\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0348 - catg_acc: 1.0000\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.0363 - catg_acc: 1.0000\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 1s 151ms/step - loss: 0.0313 - catg_acc: 1.0000\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0449 - catg_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f47d0207dd8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr_recog_model.fit(train_x, train_y, \n",
    "                          batch_size=64,epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIf acc is 1.0, it may not be 1.0.\\nIf there are multiple values for argmax,\\nPrint the leftmost value. If the dataset's label defaults to 0, then acc is 1.0 because of this.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If acc is 1.0, it may not be 1.0.\n",
    "If there are multiple values for argmax,\n",
    "Print the leftmost value. If the dataset's label defaults to 0, then acc is 1.0 because of this.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def ocr_recog_decode(res, myletters, min_th=0.2):\n",
    "    out_best=[]\n",
    "    for r in res:\n",
    "        \n",
    "        if np.max(r)>min_th:\n",
    "            argn = (np.argmax(r))\n",
    "            out_best.append(argn)\n",
    "    out_best = [k for k, g in itertools.groupby(out_best)]  # remove overlap value\n",
    "    outstr = ''\n",
    "    for i in out_best:\n",
    "        i = int(i)\n",
    "        if i < len(myletters)-1:\n",
    "            outstr += myletters[i]\n",
    "    return outstr\n",
    "\n",
    "def ocr_recog_decode_y(res, myletters, min_th=0.2):\n",
    "    \n",
    "    out_best = [k for k, g in itertools.groupby(res)]  # remove overlap value\n",
    "    outstr = ''\n",
    "    for i in out_best:\n",
    "        i = int(i)\n",
    "        if i < len(myletters)-1:\n",
    "            outstr += myletters[i]\n",
    "    return outstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "i=6\n",
    "pred_v=ocr_recog_model_pred.predict([train_x['image'][i:i+1]])\n",
    "label_v = train_x['label'][i]\n",
    "print(ocr_recog_decode_y(label_v,letters,0.1))\n",
    "print(ocr_recog_decode(pred_v[0],letters,0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: [11, 11, 11, 10, 16, 16]\n",
      "label:   [11, 11, 11, 10, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "presults=[]\n",
    "lresults=[]\n",
    "for pidx,pd in enumerate(pred_v[0]):\n",
    "    presult = np.argmax(pd)\n",
    "    lresult = label_v[pidx]#np.argmax(la[pidx])\n",
    "    \n",
    "    presults.append(presult)\n",
    "    lresults.append(lresult)\n",
    "print(\"predict:\",presults)\n",
    "print(\"label:  \",lresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
